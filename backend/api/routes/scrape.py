# backend/api/routes/scrape.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, HttpUrl
import asyncio
import hashlib
from backend.utils.playwright_scraper import scrape_website, extract_text_from_html
from backend.utils.multi_page_scraper import scrape_multiple_pages
from backend.core.vector_db import store_scraped_data
from backend.models.agent import Agent, ScrapeConfig
from datetime import datetime

router = APIRouter()

class ScrapeRequest(BaseModel):
    """Request body for scraping"""
    agent_id: str
    url: HttpUrl
    css_selector: str | None = None
    xpath: str | None = None
    multi_page: bool = False
    max_pages: int = 20
    auto_scrape: bool = False
    scrape_interval_hours: int = 24


@router.post("/scrape")
async def scrape_and_store(data: ScrapeRequest):
    """
    Scrape URL(s) and store in agent's knowledge base.
    Supports single page or multi-page crawling.
    """
    try:
        agent = Agent.get_by_id(data.agent_id)
        
        if not agent:
            raise HTTPException(status_code=404, detail=f"Agent not found: {data.agent_id}")
        
        print(f"ü§ñ Scraping for agent: {agent.name}")
        print(f"üîó URL: {data.url}")
        print(f"üìÑ Multi-page: {data.multi_page}")
        
        # Check if config exists
        existing_configs = ScrapeConfig.get_by_agent(data.agent_id)
        config_exists = any(c.url == str(data.url) for c in existing_configs)
        
        if not config_exists:
            is_primary = len(existing_configs) == 0
            config = ScrapeConfig.create(
                agent_id=data.agent_id,
                url=str(data.url),
                css_selector=data.css_selector,
                xpath=data.xpath,
                is_primary=is_primary,
                auto_scrape=data.auto_scrape,
                scrape_interval_hours=data.scrape_interval_hours
            )
            print(f"üíæ Created scrape config (auto: {data.auto_scrape}, interval: {data.scrape_interval_hours}h)")
        else:
            # Update existing config
            config = next(c for c in existing_configs if c.url == str(data.url))
            config.update(
                auto_scrape=data.auto_scrape,
                scrape_interval_hours=data.scrape_interval_hours
            )
        
        # Scrape content
        if data.multi_page:
            # Multi-page crawling
            print(f"üï∑Ô∏è Starting multi-page crawl (max: {data.max_pages} pages)")
            result = await asyncio.to_thread(
                scrape_multiple_pages,
                str(data.url),
                data.max_pages,
                data.css_selector,
                data.xpath
            )
            
            combined_text = "\n\n=== PAGE SEPARATOR ===\n\n".join([
                f"[{p['title']}]\n{p['text']}" for p in result['pages']
            ])
            
            print(f"‚úÖ Scraped {result['total_pages']} pages, {result['total_chars']:,} chars")
        else:
            # Single page
            html_content = await asyncio.to_thread(scrape_website, str(data.url))
            combined_text = extract_text_from_html(
                html_content,
                css_selector=data.css_selector,
                xpath=data.xpath
            )
            print(f"üìÑ Extracted {len(combined_text)} characters")
        
        if not combined_text.strip():
            raise HTTPException(status_code=400, detail="No text extracted")
        
        # Calculate content hash
        content_hash = hashlib.sha256(combined_text.encode()).hexdigest()
        
        # Store in vector DB
        vector_result = store_scraped_data(
            agent_id=agent.agent_id,
            url=str(data.url),
            text=combined_text,
            css_selector=data.css_selector,
            xpath=data.xpath
        )
        
        # Update config with new hash
        config.update(last_content_hash=content_hash)
        
        # Update agent
        agent.update(
            chunks_count=vector_result["chunks"],
            last_scraped=datetime.now().isoformat()
        )
        
        print(f"‚úÖ Scraping complete")
        
        return {
            "message": "Scraping successful",
            "agent": agent.to_dict(),
            "vector_db_result": vector_result,
            "pages_scraped": result['total_pages'] if data.multi_page else 1
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/scrape/refresh/{agent_id}")
async def refresh_agent_data(agent_id: str):
    """Re-scrape primary URL for an agent"""
    try:
        agent = Agent.get_by_id(agent_id)
        if not agent:
            raise HTTPException(status_code=404, detail="Agent not found")
        
        configs = ScrapeConfig.get_by_agent(agent_id)
        primary = next((c for c in configs if c.is_primary), None)
        
        if not primary:
            raise HTTPException(status_code=404, detail="No scrape config found")
        
        return await scrape_and_store(ScrapeRequest(
            agent_id=agent_id,
            url=primary.url,
            css_selector=primary.css_selector,
            xpath=primary.xpath,
            multi_page=False,
            auto_scrape=primary.auto_scrape,
            scrape_interval_hours=primary.scrape_interval_hours
        ))
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))